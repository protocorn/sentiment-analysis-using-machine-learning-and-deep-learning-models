{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNAMlLDt9RXHUoADYltIRbu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/protocorn/sentiment-analysis-using-machine-learning-and-deep-learning-models/blob/main/Cross_Domain_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading The Dataset**"
      ],
      "metadata": {
        "id": "dkJLgiuxNl89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Replace 'public_link' with your actual path to dataset\n",
        "public_link = '/kaggle/input/sentiment140/training.1600000.processed.noemoticon.csv'\n",
        "df = pd.read_csv(public_link, encoding='latin-1', header=None)"
      ],
      "metadata": {
        "id": "wPKIVQ3WNtm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preprocessing**"
      ],
      "metadata": {
        "id": "k2sXGZPJN6oh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 1 : Handling Missing Values\n",
        "\n",
        "# Drop rows with any missing values\n",
        "df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "hRnO1Sg-N_iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 2: Data Cleaning\n",
        "# Assuming the first column is the text column, convert it to lowercase and remove punctuation\n",
        "import string\n",
        "df[5] = df[5].str.lower()\n",
        "df[5] = df[5].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))"
      ],
      "metadata": {
        "id": "7P19c-vtOA_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "0Pdd054nODAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 3: Tokenization\n",
        "# Example: Tokenization\n",
        "df[5] = df[5].apply(word_tokenize)"
      ],
      "metadata": {
        "id": "Q3IBHXZhOEyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Splitting the Dataset**"
      ],
      "metadata": {
        "id": "zbHrX6GnOIpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Drop rows with empty strings in the text column\n",
        "df = df[df[5].map(bool)]\n",
        "\n",
        "X = df[5]  # Selecting the text data from the sixth column\n",
        "y = df[0]  # Selecting the target labels from the first column\n",
        "\n",
        "# Convert target labels to binary classes (0 for negative, 4 for positive)\n",
        "y = y.replace({2: 4})  # Neutral (2) is converted to positive (4)\n",
        "\n",
        "# Separate X and y again after removing empty rows\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "y_train = y_train.replace({4: 1})  # Convert positive (4) to 1\n",
        "y_test = y_test.replace({4: 1})  # Convert positive (4) to 1"
      ],
      "metadata": {
        "id": "tl7HqWO-OM6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training Machine Learning Models**"
      ],
      "metadata": {
        "id": "i_JQOHQnORH3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TF-IDF Vectorization**"
      ],
      "metadata": {
        "id": "NOt4jkmGOYRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Text Vectorization (TF-IDF)\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform([' '.join(sample) for sample in X_train])\n",
        "X_test_tfidf = tfidf_vectorizer.transform([' '.join(sample) for sample in X_test])"
      ],
      "metadata": {
        "id": "Q6_xK2uOOo1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Naive Bayes Classifier**"
      ],
      "metadata": {
        "id": "_ku9VIrlO4oO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Step 2: Model Training (Naive Bayes)\n",
        "naive_bayes_model = MultinomialNB()\n",
        "naive_bayes_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Step 3: Model Evaluation\n",
        "y_pred_naive = naive_bayes_model.predict(X_test_tfidf)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred_naive)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Generate classification report\n",
        "print(classification_report(y_test, y_pred_naive))"
      ],
      "metadata": {
        "id": "8f97q3jGOq_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Forest Classifier**"
      ],
      "metadata": {
        "id": "bhr1SC-fPOnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "random_forest_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "random_forest_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Make predictions and evaluate\n",
        "y_pred_rf = random_forest_model.predict(X_test_tfidf)\n",
        "\n",
        "y_pred_binary_rf = [1 if val > 0.5 else 0 for val in y_pred_rf]  # Convert probabilities to binary predictions\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred_binary_rf)\n",
        "print(\"Random Forest Accuracy:\", accuracy)\n",
        "\n",
        "# Generate classification report\n",
        "print(classification_report(y_test, y_pred_binary_rf))"
      ],
      "metadata": {
        "id": "P_C9rg3rO-es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logistic Regression**"
      ],
      "metadata": {
        "id": "4zExLxqnPTsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# Create and train the Logistic Regression model\n",
        "logreg_model = LogisticRegression(max_iter=1000)\n",
        "logreg_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Make predictions and evaluate\n",
        "y_pred_logistic = logreg_model.predict(X_test_tfidf)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred_logistic)\n",
        "print(\"Logistic Regression Accuracy:\", accuracy)\n",
        "print(classification_report(y_test, y_pred_logistic))"
      ],
      "metadata": {
        "id": "q40WTJjQPBst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**XGBoost**"
      ],
      "metadata": {
        "id": "_0uOhtPrPXxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "# Convert data to DMatrix\n",
        "dtrain = xgb.DMatrix(X_train_tfidf, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test_tfidf, label=y_test)\n",
        "\n",
        "# Set hyperparameters\n",
        "params = {\n",
        "    'objective': 'binary:logistic',\n",
        "    'max_depth': 6,\n",
        "    'learning_rate': 0.1,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'eval_metric': 'logloss'\n",
        "}\n",
        "\n",
        "# Train the model\n",
        "num_round = 100  # Number of boosting rounds (epochs)\n",
        "bst = xgb.train(params, dtrain, num_round)\n",
        "\n",
        "# Make predictions and evaluate\n",
        "y_pred_xgb = bst.predict(dtest)\n",
        "y_pred_binary_xgb = [1 if val > 0.5 else 0 for val in y_pred_xgb]\n",
        "accuracy = accuracy_score(y_test, y_pred_binary_xgb)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(classification_report(y_test, y_pred_binary_xgb))"
      ],
      "metadata": {
        "id": "G6BEfU8FPFXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training Deep Learning Models**"
      ],
      "metadata": {
        "id": "E2ujiZFqPmSd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization and Sequence Padding**"
      ],
      "metadata": {
        "id": "aT38KDGdPq3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Step 1: Tokenization and Padding\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([' '.join(sample) for sample in X_train])\n",
        "X_train_sequences = tokenizer.texts_to_sequences([' '.join(sample) for sample in X_train])\n",
        "X_test_sequences = tokenizer.texts_to_sequences([' '.join(sample) for sample in X_test])\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Limit the sequence length (e.g., to 100)\n",
        "max_sequence_length = 100\n",
        "\n",
        "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length)\n",
        "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length)"
      ],
      "metadata": {
        "id": "9czoz4ewP3t7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LSTM Model**"
      ],
      "metadata": {
        "id": "u1zVyzipSnDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# Step 2: Model Training (RNN)\n",
        "lstm_model = Sequential()\n",
        "lstm_model.add(Embedding(input_dim=vocab_size, output_dim=50, input_length=max_sequence_length))\n",
        "lstm_model.add(LSTM(100))\n",
        "lstm_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Use the RMSprop optimizer instead of Adam\n",
        "lstm_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "# Step 3: Model Training (Reduced Epochs and Batch Size)\n",
        "lstm_model.fit(X_train_padded, y_train, epochs=10, batch_size=32)\n",
        "\n",
        "# Step 4: Model Evaluation (after additional epochs)\n",
        "y_pred_probabilities_lstm = lstm_model.predict(X_test_padded).flatten()\n",
        "\n",
        "# Assuming y_pred_probabilities contains the predicted probabilities\n",
        "y_pred_lstm = [0 if prob <= 0.5 else 1 for prob in y_pred_probabilities_lstm]\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred_lstm)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Generate classification report\n",
        "print(classification_report(y_test, y_pred_lstm, zero_division=1))"
      ],
      "metadata": {
        "id": "omAbUeKAP6y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BiLSTM Model**"
      ],
      "metadata": {
        "id": "vLL7qEtLTF4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "\n",
        "# Define the vocabulary size and max sequence length (adjust these to your data)\n",
        "vocab_size = 10000\n",
        "max_sequence_length = 100\n",
        "\n",
        "# Create a BiLSTM model\n",
        "bilstm_model = Sequential()\n",
        "bilstm_model.add(Embedding(input_dim=vocab_size, output_dim=50, input_length=max_sequence_length))\n",
        "bilstm_model.add(Bidirectional(LSTM(100)))  # BiLSTM with 100 units\n",
        "bilstm_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the BiLSTM model (use the same optimizer and loss as the LSTM model)\n",
        "bilstm_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "# Now you can train and evaluate this BiLSTM model in a similar way as the LSTM model\n",
        "# Training without validation split\n",
        "num_epochs = 10\n",
        "batch_size = 32\n",
        "bilstm_model.fit(X_train_padded, y_train, epochs=num_epochs, batch_size=batch_size)\n",
        "\n",
        "# Evaluation on the test set\n",
        "y_pred_bilstm =bilstm_model.predict(X_test_padded)\n",
        "y_pred_binary_bilstm = (y_pred_bilstm >= 0.5).astype(int)\n",
        "accuracy = accuracy_score(y_test, y_pred_binary_bilstm)\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "print(classification_report(y_test, y_pred_binary_bilstm))"
      ],
      "metadata": {
        "id": "p5d9Es1STIuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CNN Model**"
      ],
      "metadata": {
        "id": "5qULvM0SS3pw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Prepare your text data (tokenization and padding) as before\n",
        "\n",
        "# Build the CNN model\n",
        "cnn_model = Sequential()\n",
        "cnn_model.add(Embedding(input_dim=vocab_size, output_dim=100, input_length=max_sequence_length))\n",
        "cnn_model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
        "cnn_model.add(GlobalMaxPooling1D())\n",
        "cnn_model.add(Dense(64, activation='relu'))\n",
        "cnn_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "cnn_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "cnn_model.fit(X_train_padded, y_train, epochs=10, batch_size=32)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred_cnn = cnn_model.predict(X_test_padded)\n",
        "y_pred_cnn = [1 if pred >= 0.5 else 0 for pred in y_pred_cnn]\n",
        "accuracy = accuracy_score(y_test, y_pred_cnn)\n",
        "print(\"CNN Model Accuracy:\", accuracy)\n",
        "print(classification_report(y_test, y_pred_cnn))"
      ],
      "metadata": {
        "id": "v7v6vA6iSziP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DNN Model**"
      ],
      "metadata": {
        "id": "29s5WnApTmGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
        "\n",
        "# Define your DNN model\n",
        "dnn_model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=50, input_length=max_sequence_length),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "dnn_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "# Training\n",
        "num_epochs = 10\n",
        "batch_size = 32\n",
        "dnn_model.fit(X_train_padded, y_train, epochs=num_epochs, batch_size=batch_size)\n",
        "\n",
        "# Evaluation\n",
        "accuracy = dnn_model.evaluate(X_test_padded, y_test)[1]\n",
        "print(\"Test Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "XTFIYK-xToAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading All the Trained Models**"
      ],
      "metadata": {
        "id": "ATPktZtKUyl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "logistic_regression_model = joblib.load('/kaggle/input/logistic/logistic_regression.pkl')\n",
        "bi_lstm_model =  load_model('/kaggle/input/bilstm/bilstm_model.h5')\n",
        "lstm_model =  load_model('/kaggle/input/lstm-model/lstm_model.h5')\n",
        "naive_bayes_model = joblib.load('/kaggle/input/naive-bayes/naive_bayes.pkl')\n",
        "random_forest = joblib.load('/kaggle/input/random-forest-new/random_forest1.pkl')\n",
        "cnn_model = load_model('/kaggle/input/neuralnetworks2/cnn_model.h5')\n",
        "dnn_model = load_model('/kaggle/input/neuralnetworks2/dnn_model_2.h5')\n",
        "import xgboost as xgb\n",
        "dtest = xgb.DMatrix(X_test_tfidf, label=y_test)\n",
        "xgboost_model =  xgb.Booster(model_file='/kaggle/input/xgboost/xgboost_sentiment_model.model')"
      ],
      "metadata": {
        "id": "FS4wZ1HoU2X5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Converting the .txt file to .csv format**"
      ],
      "metadata": {
        "id": "qbbTkskXVC_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Define the path to your input text file and output CSV file\n",
        "input_file_path = '/kaggle/input/sentiment-labelled-sentences-data-set/sentiment labelled sentences/amazon_cells_labelled.txt'  # Replace with the path to your input text file\n",
        "output_csv_path = '/kaggle/working/amazon_dataset.csv'  # Replace with the desired output CSV path\n",
        "\n",
        "# Initialize empty lists to store sentences and labels\n",
        "sentences = []\n",
        "labels = []\n",
        "\n",
        "# Read the input text file line by line\n",
        "with open(input_file_path, 'r') as file:\n",
        "    for line in file:\n",
        "        # Use regular expression to extract sentence and label\n",
        "        match = re.match(r'^(.*?)(\\.|!|\\?)\\s+(\\d)$', line)\n",
        "        if match:\n",
        "            sentence = match.group(1)\n",
        "            label = match.group(3)\n",
        "            sentences.append(sentence)\n",
        "            labels.append(label)\n",
        "\n",
        "# Create a DataFrame with two columns\n",
        "df2 = pd.DataFrame({'Sentence': sentences, 'Label': labels})\n",
        "\n",
        "# Write the DataFrame to a CSV file\n",
        "df2.to_csv(output_csv_path, index=False)\n",
        "\n",
        "print(f'CSV file saved to {output_csv_path}')\n",
        "\n",
        "#repeat the same process with the other dataset"
      ],
      "metadata": {
        "id": "SQCOtCEtU5D2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluating performance of Trained Models on Diverse Datasets**"
      ],
      "metadata": {
        "id": "D0exlMfyVLvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#For Machine Learning Models:\n",
        "\n",
        "def preprocess_text_with_negation_lexicon_ml(texts):\n",
        "    # If a single text is provided, convert it to a list of one element\n",
        "    if isinstance(texts, str):\n",
        "        texts = [texts]\n",
        "\n",
        "    result_texts = []\n",
        "    for text in texts:\n",
        "        # Convert to lowercase and remove punctuation\n",
        "        processed_text = text.lower()\n",
        "        processed_text = processed_text.translate(str.maketrans('', '', string.punctuation))\n",
        "        # Tokenize the text\n",
        "        tokens = word_tokenize(processed_text)\n",
        "        # Handle negation using the get_antonym function\n",
        "        result_tokens = []\n",
        "        negate = False\n",
        "        for word in tokens:\n",
        "            if word in [\"not\", \"n't\", \"no\"]:\n",
        "                negate = not negate\n",
        "            else:\n",
        "                if negate:\n",
        "                    antonym = get_antonym_ml(word)\n",
        "                    if antonym:\n",
        "                        result_tokens.append(antonym)\n",
        "                    else:\n",
        "                        result_tokens.append(\"not_\" + word)\n",
        "                    negate = False\n",
        "                else:\n",
        "                    result_tokens.append(word)\n",
        "        # Join the tokens back to a single string\n",
        "        result_texts.append(\" \".join(str(token) for token in result_tokens))\n",
        "\n",
        "    return result_texts\n",
        "\n",
        "def get_antonym_ml(word):\n",
        "    antonyms = set()\n",
        "    for synset in nltk.corpus.wordnet.synsets(word):\n",
        "        for lemma in synset.lemmas():\n",
        "            for antonym in lemma.antonyms():\n",
        "                antonyms.add(antonym.name())\n",
        "    return list(antonyms)\n",
        "\n",
        "# Step 5: Define the predict_sentiment_with_negation_lexicon function for sentiment prediction\n",
        "def predict_sentiment_with_negation_lexicon_ml(texts, model):\n",
        "    # If a single text is provided, convert it to a list of one element\n",
        "    if isinstance(texts, str):\n",
        "        texts = [texts]\n",
        "\n",
        "    result_texts = []\n",
        "    for text in texts:\n",
        "        processed_text = preprocess_text_with_negation_lexicon_ml(text)\n",
        "        result_texts.append(\" \".join(processed_text))  # Join the list of tokens back to a single string\n",
        "\n",
        "    # Vectorize the preprocessed text using the pre-trained TF-IDF vectorizer\n",
        "    import xgboost as xgb\n",
        "\n",
        "    processed_text_tfidf = tfidf_vectorizer.transform(result_texts)\n",
        "\n",
        "    # Convert the processed_text_tfidf to a DMatrix\n",
        "    #d_test = xgb.DMatrix(processed_text_tfidf)\n",
        "    #uncomment the above line only when working with XGBoost Model\n",
        "\n",
        "    predicted_sentiments = model.predict(processed_text_tfidf) #use this for all models except for the XGBoost\n",
        "\n",
        "    #predicted_sentiments = model.predict(d_test)\n",
        "    #uncomment the above line only when working with XGBoost Model\n",
        "\n",
        "    return predicted_sentiments"
      ],
      "metadata": {
        "id": "c0ckgXKhVW-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For Deep Learning Models:\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def preprocess_text_with_negation_lexicon(texts):\n",
        "    # If a single text is provided, convert it to a list of one element\n",
        "    if isinstance(texts, str):\n",
        "        texts = [texts]\n",
        "\n",
        "    result_texts = []\n",
        "    for text in texts:\n",
        "        # Convert to lowercase and remove punctuation\n",
        "        processed_text = text.lower()\n",
        "        processed_text = processed_text.translate(str.maketrans('', '', string.punctuation))\n",
        "        # Tokenize the text by splitting it into words\n",
        "        tokens = processed_text.split()\n",
        "        # Handle negation using the get_antonym function\n",
        "        result_tokens = []\n",
        "        negate = False\n",
        "        for word in tokens:\n",
        "            if word in [\"not\", \"n't\", \"no\"]:\n",
        "                negate = not negate\n",
        "            else:\n",
        "                if negate:\n",
        "                    antonym = get_antonym(word)\n",
        "                    if antonym:\n",
        "                        result_tokens.append(antonym[0])  # Append the first antonym\n",
        "                    else:\n",
        "                        result_tokens.append(\"not_\" + word)\n",
        "                    negate = False\n",
        "                else:\n",
        "                    result_tokens.append(word)\n",
        "        # Join the tokens back to a single string\n",
        "        result_texts.append(\" \".join(result_tokens))\n",
        "\n",
        "    return result_texts\n",
        "\n",
        "def get_antonym(word):\n",
        "    antonyms = []\n",
        "    for synset in nltk.corpus.wordnet.synsets(word):\n",
        "        for lemma in synset.lemmas():\n",
        "            for antonym in lemma.antonyms():\n",
        "                antonyms.append(antonym.name())\n",
        "    return antonyms\n",
        "\n",
        "def predict_sentiment_with_negation_lexicon(texts, model, tokenizer, max_sequence_length):\n",
        "    if isinstance(texts, str):\n",
        "        texts = [texts]\n",
        "\n",
        "    predicted_sentiments = []\n",
        "    for text in texts:\n",
        "        # Preprocess text with negation handling and tokenization\n",
        "        processed_text = preprocess_text_with_negation_lexicon(text)\n",
        "        sequences = tokenizer.texts_to_sequences(processed_text)\n",
        "        padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "\n",
        "        # Predict sentiment using the deep learning models\n",
        "        predicted_sentiment = model.predict(padded_sequences)\n",
        "        predicted_sentiments.append(predicted_sentiment[0])\n",
        "\n",
        "    return predicted_sentiments"
      ],
      "metadata": {
        "id": "sc8K9il9V24x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.iloc[:, 1] = df2.iloc[:, 1].astype(int)"
      ],
      "metadata": {
        "id": "D9MS1HlnV7VG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_texts_ml = df2.iloc[:, 0].tolist()\n",
        "actual_labels = df2.iloc[:, 1].tolist()"
      ],
      "metadata": {
        "id": "5YRODs-wV71i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prediction using Mahcine Learning Models**"
      ],
      "metadata": {
        "id": "xKSvJtdJWVAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ml_model = random_forest #you can replace ml_model with any machine learning models we have trained\n",
        "predicted_sentiments_ml = predict_sentiment_with_negation_lexicon_ml(new_texts_ml, ml_model)"
      ],
      "metadata": {
        "id": "iSdjjAVlV-p1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy by comparing predicted labels to actual labels\n",
        "predicted_sentiments_ml = [1 if pred >= 0.5 else 0 for pred in predicted_sentiments_ml]\n",
        "correct_predictions = sum(1 for predicted, actual in zip(predicted_sentiments_ml, actual_labels) if predicted == actual)\n",
        "total_predictions = len(predicted_sentiments_ml)\n",
        "accuracy = correct_predictions / total_predictions * 100\n",
        "print(f\"Accuracy on the new dataset: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "OLUTSj1AWKAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prediction Using Deep Learning Models**"
      ],
      "metadata": {
        "id": "nkaN2gWDWah1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Split the data into batches\n",
        "batch_size = 100  # Adjust the batch size as needed\n",
        "num_batches = len(new_texts_ml) // batch_size\n",
        "\n",
        "predicted_sentiments_dl = []  # List to store predicted sentiments\n",
        "\n",
        "dl_model = dnn_model #replace the dnn_model with any deep learning model if required\n",
        "\n",
        "for i in range(num_batches):\n",
        "    start_idx = i * batch_size\n",
        "    end_idx = (i + 1) * batch_size\n",
        "    batch_texts = new_texts_ml[start_idx:end_idx]\n",
        "\n",
        "    # Call the function to get predicted sentiments for the batch\n",
        "    predicted_batch_sentiments = predict_sentiment_with_negation_lexicon(\n",
        "        batch_texts, dl_model, tokenizer, max_sequence_length\n",
        "    )\n",
        "\n",
        "    # Append the batch predictions to the list\n",
        "    predicted_sentiments_dl.extend(predicted_batch_sentiments)\n",
        "\n",
        "# Handle any remaining data that doesn't fit in a full batch\n",
        "if len(new_texts_ml) % batch_size != 0:\n",
        "    remaining_texts = new_texts_ml[num_batches * batch_size :]\n",
        "    predicted_remaining_sentiments = predict_sentiment_with_negation_lexicon(\n",
        "        remaining_texts, dl_model, tokenizer, max_sequence_length\n",
        "    )\n",
        "    predicted_sentiments_dl.extend(predicted_remaining_sentiments)"
      ],
      "metadata": {
        "id": "jtaWQydqWd0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_sentiments_dl = [1 if pred >= 0.5 else 0 for pred in predicted_sentiments_dl]\n",
        "# Calculate accuracy by comparing predicted labels to actual labels\n",
        "correct_predictions = sum(1 for predicted, actual in zip(predicted_sentiments_dl, actual_labels) if predicted == actual)\n",
        "total_predictions = len(predicted_sentiments_dl)\n",
        "accuracy = correct_predictions / total_predictions * 100\n",
        "\n",
        "print(f\"Accuracy on the new dataset: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "787LLScqWoeG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}